# Intro
Lorem Ipsum

## The Very High-level Stuff

### What is an LLM?

### Is ChatGPT an LLM?

### What can an LLM do for me?

### What can't LLMs do? (particularly well. yet.)

### Can I run my own LLM?

### Why would I want to run my own LLM?

### What is required to run my own LLM at home?
Yes. In order to......

### Can I run my own LLM in the Cloud?
Yes. There are plenty options.....

### How is the quality of an LLM measured?
Also known as Benchmarking, statistics and other lies.

### How is the performance of an LLM measured?
More Benchmarking, statistics and other lies.


## Hardware for hosting a local LLM

### High-level overview - what performance *matters* for LLMs?

Any computer has a multitude of bottlenecks. If we focus on the hardware *only*, the following is a good list:
* storage
* net
* memory
* CPU
* GPU
* interconnects (buses)

And while a sports car is both fast *and* quick, it may not have a lot of capacity for moving *lots of stuff*. In other words, the actual bottleneck may be either one of:
* speed
* latency
* capacity

Lorem ipsum.......

### CPU vs GPU

### CPU

### GPU

### Motherboard

### Memory



## Basic Terminology

### LLM families

### LLM formats

### LLM quantization

### Loader

### Front-end

### Prompt

### Training

### Inference

### Context

### API


## Medium Terminology

### Perplexity

### Merged model

### 













